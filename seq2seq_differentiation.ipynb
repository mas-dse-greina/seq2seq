{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence prediction\n",
    "\n",
    "We'll create a seq2seq LSTM model in Keras which will predict one sequence based on another. In this case, the input sequence is a random permutation of numbers from 1 to 9. The output sequence is the difference between every successive value in the input sequence (with 0 to pad the output sequence at the start)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/anaconda3/envs/tf/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters\n",
    "\n",
    "We assume that the only valid sequences contain numbers from 1 to 9. Additionally, we know that since the output sequence is the difference between any two input values, then the output values can only range from -18 to +18. However, let's make our dictionary only go from -17 to +17. We'll use the \"unknown\" class when the values are outside of our known dictionary (e.g. -18 and +18). We'll also use a pad in case we want the input and output sequences to vary in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_timesteps_in = 10   # Number of values in the input sequence\n",
    "n_timesteps_out = 8   # Number of values in the output sequence \n",
    "\n",
    "n_hidden_units = 200  # Number of hidden units in the LSTM\n",
    "\n",
    "int_low = 1           # Minimum value possible in a legal input sequence\n",
    "int_high = 9          # Maxmum value possible in a legal input sequence\n",
    "pad_value = -9999     # Value of pad (end of the sequence)\n",
    "unknown_value = 9999  # Value of anything not in the expected vocabulary\n",
    "\n",
    "legal_values = np.arange(int_low, int_high+1)  # [1-9]\n",
    "\n",
    "# Integers from -17 to +17. These will actually be consider \"classes\" not \"numbers\".\n",
    "# We intentionally skip -18 and +18 to show how model can predict \"unknown\".\n",
    "all_values = np.arange(1-int_high*2,int_high*2) \n",
    "# Append pad and unknown value classes\n",
    "all_values = np.append(all_values, [pad_value, unknown_value])\n",
    "\n",
    "n_features = len(all_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the data\n",
    "Here we generate the data for the model. Obviously, in a real world scenario we'd be given the input and output data. We need to one hot encode the input and output sequences in order to use categorical cross-entropy as the loss function. We also add padding randomly to the input sequence so that the model can handle variable length inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sequence(length):\n",
    "    '''\n",
    "    Generate a random vector of values from the legal vector of values\n",
    "    e.g.  [2, 3, 1, 4, 1, 9, 5, 2]\n",
    "    We'll randomly make the last few entries the pad so that the model\n",
    "    can handle random sized sequences.\n",
    "    '''\n",
    "    out = np.random.choice(legal_values, length)  # Random vector of legal values (with replacement)\n",
    "    pad = np.random.randint(0,3)\n",
    "    if pad > 0:\n",
    "        out[-pad:] = pad_value\n",
    "    return out\n",
    "\n",
    "def one_hot_encode(sequence):\n",
    "    '''\n",
    "    Convert a vector into a one hot encoded matrix using all possible classes\n",
    "    in our vocabulary.\n",
    "    '''\n",
    "    encoding = []\n",
    "    for value in sequence:\n",
    "        vector = np.zeros_like(all_values)\n",
    "        if value in all_values:\n",
    "            vector[np.where(all_values==value)[0]] = 1\n",
    "        else:\n",
    "            vector[np.where(all_values==unknown_value)[0]] = 1\n",
    "        encoding.append(vector)\n",
    "        \n",
    "    encoding = np.array(encoding)\n",
    "    encoding = encoding.reshape(1, encoding.shape[0], encoding.shape[1])\n",
    "    return np.array(encoding)\n",
    "\n",
    "def one_hot_decode(encoded_seq):\n",
    "    '''\n",
    "    Convert the one hot encoding back into the original vocabulary space.\n",
    "    '''\n",
    "    return [all_values[np.argmax(vector)] for vector in encoded_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_sequence(sequence):\n",
    "    '''\n",
    "    Transform the sequence from the input to the output\n",
    "    Here we are doing differentiation.\n",
    "    For example, if the input is [3,  2, 5, 5,  2, 6]\n",
    "    then the output is:          [0, -1, 3, 0, -3, 4]\n",
    "    The output is always the difference between two successive input values.\n",
    "    '''\n",
    "    return np.insert(np.diff(sequence), 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(n_in, n_out):\n",
    "    '''\n",
    "    Generate a random sequence of input and outputs.\n",
    "    Return the one hot encoded sequences.\n",
    "    '''\n",
    "    sequence_in = generate_sequence(n_in)\n",
    "    sequence_transform = transform_sequence(sequence_in)\n",
    "    sequence_out = np.ones_like(sequence_in)*pad_value\n",
    "    sequence_out[:n_out] = sequence_transform[:n_out]\n",
    "    \n",
    "    # One Hot Encode the input values\n",
    "    X = one_hot_encode(sequence_in)\n",
    "    y = one_hot_encode(sequence_out)\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator\n",
    "This is a simple data generator to keep pulling random batches for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(n_in, n_out, batch_size):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        for idx in range(batch_size):\n",
    "            \n",
    "            X,y = get_data(n_in, n_out)\n",
    "            X_batch.append(X[0])\n",
    "            y_batch.append(y[0])\n",
    "            \n",
    "        yield np.array(X_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq model\n",
    "\n",
    "Here's the entire sequence to sequence model using an encoder of LSTM and a decoder of LSTM. We only pass the last hidden state of the encoder to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_hidden_units, input_shape=(n_timesteps_in, n_features)))\n",
    "model.add(RepeatVector(n_timesteps_in))\n",
    "model.add(LSTM(n_hidden_units, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_features, activation=\"softmax\")))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256         # Batch size for training\n",
    "num_epochs = 15          # Number of epochs\n",
    "training_steps = 1000    # Number of training steps per epoch\n",
    "validation_steps = 100   # Number of validation steps\n",
    "\n",
    "# Create a batch generator for the training data\n",
    "train_generator = get_batch(n_timesteps_in, n_timesteps_out, batch_size)\n",
    "\n",
    "# Create a batch generator for the validation data\n",
    "validate_generator = get_batch(n_timesteps_in, n_timesteps_out, batch_size)\n",
    "\n",
    "# Create callbacks for model saving and TensorBoard\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"seq2seq_model.h5\", monitor=\"val_loss\", \n",
    "                                verbose=0, save_best_only=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./tb_logs\", write_graph=True)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, \n",
    "                                               verbose=0, mode=\"auto\")\n",
    "\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=training_steps, \n",
    "                              epochs=num_epochs, \n",
    "                              validation_data=validate_generator, \n",
    "                              validation_steps=validation_steps,\n",
    "                              verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print some test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check a few example predictions to sanity check trained model\n",
    "num_examples=10\n",
    "for idx in range(num_examples):\n",
    "    X,y = get_pair(n_timesteps_in, n_timesteps_out)\n",
    "    yhat = model.predict(X, verbose=0)\n",
    "    print(\"*\"*20)\n",
    "    print(\"Test case #{}\".format(idx+1))\n",
    "    print(\"Input = \", one_hot_decode(X[0]))\n",
    "    print(\"Expected Output:\", one_hot_decode(y[0]))\n",
    "    print(\"Model Predicted Output:\", one_hot_decode(yhat[0]))\n",
    "    print(\"*\"*20)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a manual test case\n",
    "\n",
    "Enter in your own sequence to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = one_hot_encode([3,2, 4,5,-3,2,1,4])\n",
    "yhat = model.predict(X, verbose=0)\n",
    "print(\"*\"*20)\n",
    "print(\"Manual Test Case\".format(idx+1))\n",
    "print(\"Input = \", one_hot_decode(X[0]))\n",
    "print(\"Expected Output:\", one_hot_decode(y[0]))\n",
    "print(\"Model Predicted Output:\", one_hot_decode(yhat[0]))\n",
    "print(\"*\"*20)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
